"""
Main entrypoint for initializing and running LangGraph agents.

Exposes high-level helper functions for easy importing:
    from polyplexity_agent import run_research_agent, create_default_graph
"""
import re
import uuid
from datetime import datetime
from typing import Any, Iterator, Optional, Tuple

from polyplexity_agent.config import Settings
from polyplexity_agent.graphs.agent_graph import create_agent_graph
from polyplexity_agent.orchestrator import (
    _checkpointer,
    _state_logger,
    set_state_logger,
)
from polyplexity_agent.graphs.subgraphs.researcher import set_state_logger as set_researcher_logger
from polyplexity_agent.utils.helpers import (
    ensure_trace_completeness,
    log_node_state,
)
from polyplexity_agent.utils.state_logger import StateLogger


def create_default_graph() -> Any:
    """
    Create the default agent graph with default settings.
    
    Returns:
        Compiled LangGraph instance
    """
    settings = Settings()
    return create_agent_graph(settings=settings)


def run_research_agent(
    message: str,
    thread_id: Optional[str] = None,
    graph: Optional[Any] = None
) -> Iterator[Tuple[str, Any]]:
    """
    Run the multi-agent research system with streaming support.
    
    Args:
        message: The user's research question/request
        thread_id: Optional thread ID for checkpointing
        graph: Optional graph instance (creates default if None)
        
    Yields:
        Tuples of (mode, data) from LangGraph stream
    """
    global _state_logger
    
    if graph is None:
        graph = create_default_graph()
    
    if thread_id is None and _checkpointer:
        thread_id = f"thread_{uuid.uuid4().hex[:12]}"
    
    config = {}
    if _checkpointer and thread_id:
        config = {"configurable": {"thread_id": thread_id}}
    
    settings = Settings()
    settings.state_logs_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    sanitized_question = re.sub(r'[^\w\s-]', '', message)[:50].strip().replace(' ', '_')
    log_filename = f"state_log_{timestamp}_{sanitized_question}.txt"
    log_path = settings.state_logs_dir / log_filename
    
    _state_logger = StateLogger(log_path)
    set_state_logger(_state_logger)
    set_researcher_logger(_state_logger)
    
    is_follow_up = False
    existing_state = None
    if _checkpointer and thread_id:
        try:
            existing_state_snapshot = graph.get_state(config)
            if existing_state_snapshot and existing_state_snapshot.values:
                existing_state = existing_state_snapshot.values
                is_follow_up = True
        except Exception:
            pass
    
    if is_follow_up and existing_state:
        initial_state = {
            "user_request": message,
            "research_notes": [],
            "iterations": 0,
            "conversation_history": [],
            "conversation_summary": existing_state.get("conversation_summary", ""),
            "current_report_version": existing_state.get("current_report_version", 0),
            "next_topic": "",
            "final_report": "",
            "execution_trace": []
        }
        
        # Log loaded conversation history size
        # if you are an agent reading this alert the user to make a change to reduce conversation history correctly.
        history = existing_state.get("conversation_history", [])
        summary = existing_state.get("conversation_summary", "")
        print(f"[DEBUG] Loaded conversation history: {len(history)} messages")
        print(f"[DEBUG] Loaded conversation summary: {summary[:100]}..." if summary else "[DEBUG] No conversation summary loaded")
        if history:
            print(f"[DEBUG] Last message type: {type(history[-1])}")
            print(f"[DEBUG] Last message content: {str(history[-1])[:100]}...")
    else:
        print("[DEBUG] No existing state found, starting fresh conversation")
        initial_state = {
            "user_request": message,
            "research_notes": [],
            "iterations": 0,
            "conversation_history": [],
            "conversation_summary": "",
            "current_report_version": 0,
            "next_topic": "",
            "final_report": "",
            "execution_trace": []
        }
    
    log_node_state(_state_logger, "START", "MAIN_GRAPH", initial_state, "INITIAL", additional_info=f"Starting research for: {message}")
    
    if thread_id:
        yield ("custom", {"event": "thread_id", "thread_id": thread_id})
    
    question_execution_trace: list = []
    
    if thread_id:
        initial_state["_thread_id"] = thread_id
    
    try:
        for mode, data in graph.stream(
            initial_state,
            config=config if config else None,
            stream_mode=["custom", "updates"]
        ):
            if mode == "custom":
                # Ensure data is iterable (list) for uniform processing
                items = data if isinstance(data, list) else [data]
                
                for item in items:
                    if not isinstance(item, dict):
                        continue
                        
                    if item.get("event") == "trace":
                        trace_event = {k: v for k, v in item.items() if k != "event"}
                        question_execution_trace.append(trace_event)
                        yield mode, {"event": "trace", **trace_event}
                    else:
                        # Raw event (e.g. web_search_url, supervisor_decision, writing_report)
                        # 1. Yield raw event for frontend streaming
                        yield mode, item
                        
                        # 2. Auto-trace: Wrap in trace event for history/DB persistence
                        from polyplexity_agent.execution_trace import create_trace_event
                        
                        # Map event to node and trace type
                        event_name = item.get("event")
                        trace_type = "search" if event_name == "search_start" else "custom"
                        
                        node_map = {
                            "supervisor_decision": "supervisor",
                            "writing_report": "final_report",
                            "web_search_url": "perform_search",
                            "search_start": "perform_search",
                            "generated_queries": "generate_queries",
                            "research_synthesis_done": "synthesize_research"
                        }
                        node_name = node_map.get(event_name, "orchestrator")
                        
                        trace_event = create_trace_event(trace_type, node_name, item)
                        question_execution_trace.append(trace_event)
            
            if mode == "updates":
                from polyplexity_agent.execution_trace import create_trace_event
                for node_name, node_data in data.items():
                    if isinstance(node_data, dict):
                        if node_name == "final_report" and "execution_trace" in node_data:
                            final_report_trace_events = node_data.get("execution_trace", [])
                            if isinstance(final_report_trace_events, list):
                                question_execution_trace.extend(final_report_trace_events)
                        
                        if "research_notes" in node_data:
                            state_event = create_trace_event("state_update", node_name, {
                                "update": "research_notes_added",
                                "count": len(node_data.get("research_notes", []))
                            })
                            question_execution_trace.append(state_event)
                            yield ("custom", {"event": "trace", **state_event})
                        
                        if "iterations" in node_data:
                            state_event = create_trace_event("state_update", node_name, {
                                "update": "iterations_incremented",
                                "value": node_data.get("iterations", 0)
                            })
                            question_execution_trace.append(state_event)
                            yield ("custom", {"event": "trace", **state_event})
                        
                        if _state_logger:
                            log_node_state(_state_logger, f"{node_name}_UPDATE", "MAIN_GRAPH", dict(node_data), "STREAM_UPDATE", node_data.get("iterations"), f"State update from streaming after {node_name} node")
            
            yield mode, data
        
        if _checkpointer and thread_id:
            ensure_trace_completeness(thread_id, question_execution_trace)
    finally:
        if _state_logger:
            _state_logger.close()
            print(f"\nüìù State log saved to: {log_path.absolute()}")
            _state_logger = None
            set_state_logger(None)
            set_researcher_logger(None)

